{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/promckkon/MK-DCNN/blob/main/MK-DCNN%20with%200dB%20NOISE%20in%20CWRU%20Dataset%2026124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JTcgOD9s-GR",
        "outputId": "b1bdf052-4df6-4b56-c07d-359f6bdc7768"
      },
      "id": "_JTcgOD9s-GR",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import catboost\n",
        "import sklearn\n",
        "\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"catboost:\", catboost.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iGFuoIV0NjQ",
        "outputId": "5aa867f6-5f6c-486b-85f8-bb8bb7b4105a"
      },
      "id": "1iGFuoIV0NjQ",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.26.4\n",
            "catboost: 1.2.7\n",
            "sklearn: 1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y catboost\n",
        "!pip -q install -U \"numpy==1.26.4\"\n",
        "!pip -q install --no-cache-dir --force-reinstall \"catboost==1.2.7\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjQWrwh30I6W",
        "outputId": "b6f84e95-2692-43cb-bead-c8589397758f"
      },
      "id": "NjQWrwh30I6W",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m338.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m360.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m248.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m355.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m225.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m268.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m231.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m272.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m280.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m297.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m307.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m296.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m418.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.9/432.9 kB\u001b[0m \u001b[31m427.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m351.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m276.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m340.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m386.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.53.0 requires pandas<3,>=1.4.0, but you have pandas 3.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "langchain-core 1.2.7 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.1 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.19 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "1132b641",
      "metadata": {
        "id": "1132b641"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Input"
      ],
      "metadata": {
        "id": "2haV_hKQQOg-"
      },
      "id": "2haV_hKQQOg-"
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(columns=['DE_data','fault']) # upload the dataset on googledrive\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2\", topdown=False):\n",
        "    for file_name in files:\n",
        "        path = os.path.join(root, file_name)\n",
        "        print(path)\n",
        "\n",
        "        mat = scipy.io.loadmat(path)\n",
        "\n",
        "        key_name = list(mat.keys())[3]\n",
        "        DE_data = mat.get(key_name)\n",
        "        fault = np.full((len(DE_data), 1), file_name[:-4])\n",
        "\n",
        "        df_temp = pd.DataFrame({'DE_data':np.ravel(DE_data) , 'fault':np.ravel(fault)})\n",
        "\n",
        "        df = pd.concat([df,df_temp],axis=0)\n",
        "        print(df['fault'].unique())\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/MK-DCNN CWRU/NOISE_0_faults.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXzSVyy_ylsF",
        "outputId": "a52749ee-3d3a-4c32-ff4f-29907a1eba3b"
      },
      "id": "sXzSVyy_ylsF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F7_OuterRace_7.mat\n",
            "['F7_OuterRace_7']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F6_InnerRace_21.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4247073321.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df,df_temp],axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F7_OuterRace_7' 'F6_InnerRace_21']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F2_Ball_14.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F4_InnerRace_7.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F0_Normal.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F8_OuterRace_14.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal' 'F8_OuterRace_14']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F5_InnerRace_14.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal' 'F8_OuterRace_14' 'F5_InnerRace_14']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F1_Ball_7.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal' 'F8_OuterRace_14' 'F5_InnerRace_14' 'F1_Ball_7']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F3_Ball_21.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal' 'F8_OuterRace_14' 'F5_InnerRace_14' 'F1_Ball_7' 'F3_Ball_21']\n",
            "/content/drive/MyDrive/CWRU_with_NOISE/CWRU_2/F9_OuterRace_21.mat\n",
            "['F7_OuterRace_7' 'F6_InnerRace_21' 'F2_Ball_14' 'F4_InnerRace_7'\n",
            " 'F0_Normal' 'F8_OuterRace_14' 'F5_InnerRace_14' 'F1_Ball_7' 'F3_Ball_21'\n",
            " 'F9_OuterRace_21']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Kernel Deep 1D-CNN"
      ],
      "metadata": {
        "id": "2d0arbB2knS8"
      },
      "id": "2d0arbB2knS8"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "-dsJA9B5YVAW"
      },
      "id": "-dsJA9B5YVAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Total target rows\n",
        "TARGET_ROWS = 1800\n",
        "\n",
        "# Estimate the total number of samples across all fault types\n",
        "total_samples = sum(len(df[df['fault'] == f]) for f in df['fault'].unique())\n",
        "\n",
        "# Calculate suitable window size and stride\n",
        "num_faults = 9\n",
        "average_samples_per_fault = total_samples / num_faults\n",
        "approx_windows_per_fault = TARGET_ROWS / num_faults\n",
        "stride_ratio = 0.8  # Initial stride/window ratio\n",
        "\n",
        "win_len = int(average_samples_per_fault / approx_windows_per_fault)\n",
        "stride = int(win_len * stride_ratio)\n",
        "\n",
        "X=[]\n",
        "Y=[]\n",
        "\n",
        "\n",
        "for k in df['fault'].unique():\n",
        "\n",
        "    df_temp_2 = df[df['fault']==k]\n",
        "\n",
        "    for i in np.arange(0,len(df_temp_2)-(win_len),stride):\n",
        "        temp = df_temp_2.iloc[i:i+win_len,:-1].values\n",
        "        temp = temp.reshape((1,-1))\n",
        "        X.append(temp)\n",
        "        Y.append(df_temp_2.iloc[i+win_len,-1])\n",
        "\n",
        "X=np.array(X)\n",
        "X=X.reshape((X.shape[0],-1,1))\n",
        "#X = np.repeat(X, 3, axis=3) # To repeat into 3 chanel format\n",
        "\n",
        "\n",
        "Y=np.array(Y)\n",
        "encoder= LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "OHE_Y = to_categorical(encoded_Y)"
      ],
      "metadata": {
        "id": "5s5vUXrUM2BH"
      },
      "id": "5s5vUXrUM2BH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,OHE_Y,test_size=0.3,shuffle=True)"
      ],
      "metadata": {
        "id": "PreHyG2yMkTA"
      },
      "id": "PreHyG2yMkTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Custom Physics-Informed Loss Function ---\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Standard categorical crossentropy\n",
        "    loss = K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Physics-Informed Term: penalize rapid class probability changes\n",
        "    # Ensure y_pred is at least 2D for slicing\n",
        "    if K.ndim(y_pred) < 2:\n",
        "        y_pred = K.expand_dims(y_pred, axis=-1)\n",
        "\n",
        "    # Ensure y_pred has more than one class dimension to compute diff\n",
        "    if K.int_shape(y_pred)[-1] > 1:\n",
        "        diff = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "        squared_diff = tf.square(diff)\n",
        "        physics_term = tf.reduce_mean(squared_diff)\n",
        "    else:\n",
        "        physics_term = 0.0 # No physics term if only one class\n",
        "\n",
        "\n",
        "    # Total loss = classification loss + regularization term\n",
        "    total_loss = loss + 0.01 * physics_term  # 0.01 is tunable\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# --- Model Definition ---\n",
        "# no_classes = len(df['fault'].unique()) # Original line that caused 10 classes\n",
        "no_classes = len(encoder.classes_) # Use the encoder to get the correct number of classes\n",
        "print(f\"Number of output classes (no_classes): {no_classes}\") # Add print statement to verify\n",
        "\n",
        "input_shape = (X.shape[1], X.shape[2])  # Example: (784, 1)\n",
        "\n",
        "# Head 1\n",
        "inputs1 = Input(shape=input_shape)\n",
        "conv1 = Conv1D(filters=64, kernel_size=200, activation='relu')(inputs1)\n",
        "drop1 = Dropout(0.5)(conv1)\n",
        "pool1 = MaxPooling1D(pool_size=20)(drop1)\n",
        "flat1 = Flatten()(pool1)\n",
        "\n",
        "# Head 2\n",
        "inputs2 = Input(shape=input_shape)\n",
        "conv2 = Conv1D(filters=64, kernel_size=100, activation='relu')(inputs2)\n",
        "drop2 = Dropout(0.5)(conv2)\n",
        "pool2 = MaxPooling1D(pool_size=10)(drop2)\n",
        "flat2 = Flatten()(pool2)\n",
        "\n",
        "# Head 3\n",
        "inputs3 = Input(shape=input_shape)\n",
        "conv3 = Conv1D(filters=64, kernel_size=50, activation='relu')(inputs3)\n",
        "drop3 = Dropout(0.5)(conv3)\n",
        "pool3 = MaxPooling1D(pool_size=5)(drop3)\n",
        "flat3 = Flatten()(pool3)\n",
        "\n",
        "# Merge all feature paths\n",
        "merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "# Fully connected interpretation\n",
        "dense1 = Dense(100, activation='relu')(merged)\n",
        "outputs = Dense(no_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Build and compile model with custom loss\n",
        "cnn_model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "cnn_model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "kB3uGh-8J7no"
      },
      "id": "kB3uGh-8J7no",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =100\n",
        "epochs = 20\n",
        "\n",
        "# Explicitly convert X_train and X_test to a numeric dtype if they are object dtype\n",
        "# This addresses the ValueError: Invalid dtype: object\n",
        "if X_train.dtype == 'object':\n",
        "    X_train = X_train.astype(np.float32)\n",
        "if X_test.dtype == 'object':\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "history = cnn_model.fit([X_train,X_train,X_train], y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=([X_test,X_test,X_test],y_test),shuffle=True)"
      ],
      "metadata": {
        "id": "2Lxm-qvKM3oN"
      },
      "id": "2Lxm-qvKM3oN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inv_Transform_result(y_pred):\n",
        "    y_pred = y_pred.argmax(axis=1)\n",
        "    y_pred = encoder.inverse_transform(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "y_pred=cnn_model.predict([X_test,X_test,X_test])\n",
        "\n",
        "\n",
        "Y_pred=inv_Transform_result(y_pred)\n",
        "Y_test = inv_Transform_result(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "cm = confusion_matrix(Y_test, Y_pred,normalize='true')\n",
        "f = sns.heatmap(cm, annot=True,xticklabels=encoder.classes_,yticklabels=encoder.classes_)"
      ],
      "metadata": {
        "id": "pbBmkBjlM5ks"
      },
      "id": "pbBmkBjlM5ks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cnn = Model(inputs=cnn_model.input,outputs=cnn_model.layers[16].output)\n",
        "y_viz = dummy_cnn.predict([X_train,X_train,X_train])"
      ],
      "metadata": {
        "id": "3tgRx2TxM-GS"
      },
      "id": "3tgRx2TxM-GS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Apply t-SNE transformation\n",
        "X_t_sne = TSNE(\n",
        "    n_components=2,\n",
        "    learning_rate='auto',\n",
        "    verbose=2,  # Increase verbosity for more detailed feedback\n",
        "    perplexity=40\n",
        ").fit_transform(y_viz)\n",
        "\n",
        "# Create DataFrame for t-SNE components\n",
        "tSNEdf = pd.DataFrame(data=X_t_sne, columns=['T-SNE component 1', 'T-SNE component 2'])\n",
        "\n",
        "# Assuming `inv_Transform_result` transforms `y_train` to get the Fault labels\n",
        "tSNEdf['Fault'] = inv_Transform_result(y_train)\n",
        "\n",
        "# Save the t-SNE DataFrame as a CSV file\n",
        "tSNEdf.to_csv('/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_tSNE_results.csv', index=True)\n",
        "print(\"t-SNE results saved as '/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_tSNE_results.csv'.\")\n",
        "\n",
        "# # Plot the t-SNE results with Fault as hue\n",
        "# plt.figure(figsize=(12, 12))  # Larger figure for clarity\n",
        "# sns.scatterplot(\n",
        "#     x='T-SNE component 1',\n",
        "#     y='T-SNE component 2',\n",
        "#     hue='Fault',\n",
        "#     palette=sns.color_palette(\"husl\", as_cmap=False),  # Vivid color palette\n",
        "#     data=tSNEdf,\n",
        "#     legend=\"full\",\n",
        "#     alpha=0.7,  # Increased transparency for better overlap clarity\n",
        "#     s=100  # Larger marker size\n",
        "# )\n",
        "\n",
        "# # Add gridlines for better visualization\n",
        "# plt.grid\n",
        "\n",
        "\n",
        "# Optional: Visualization of t-SNE representation\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in tSNEdf['Fault'].unique():\n",
        "    subset = tSNEdf[tSNEdf['Fault'] == label]\n",
        "    plt.scatter(subset['T-SNE component 1'], subset['T-SNE component 2'], label=f'Fault {label}', alpha=0.7)\n",
        "\n",
        "plt.title('t-SNE Representation of Motor Fault Dataset')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uic9AVV2NBah"
      },
      "id": "uic9AVV2NBah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Features"
      ],
      "metadata": {
        "id": "PRuXPOIH6cTH"
      },
      "id": "PRuXPOIH6cTH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Total target rows\n",
        "TARGET_ROWS = 1800\n",
        "\n",
        "# Estimate the total number of samples across all fault types\n",
        "total_samples = sum(len(df[df['fault'] == f]) for f in df['fault'].unique())\n",
        "\n",
        "# Calculate suitable window size and stride\n",
        "num_faults = 9\n",
        "average_samples_per_fault = total_samples / num_faults\n",
        "approx_windows_per_fault = TARGET_ROWS / num_faults\n",
        "stride_ratio = 0.8  # Initial stride/window ratio\n",
        "\n",
        "WINDOW_SIZE = int(average_samples_per_fault / approx_windows_per_fault)\n",
        "STRIDE = int(WINDOW_SIZE * stride_ratio)\n",
        "\n",
        "print(f\"Calculated WINDOW_SIZE: {WINDOW_SIZE}, STRIDE: {STRIDE}\")\n",
        "\n",
        "# Initialize a list to store statistical features\n",
        "statistical_features = []\n",
        "\n",
        "# Loop through each fault type\n",
        "for f in df['fault'].unique():\n",
        "    # Extract data for the current fault type\n",
        "    # Convert to NumPy array for efficiency and ensure float type\n",
        "    fault_data = df[df['fault'] == f].iloc[:, 0].values.astype(float)\n",
        "\n",
        "    # Compute windows using sliding window approach\n",
        "    num_windows = (len(fault_data) - WINDOW_SIZE) // STRIDE + 1\n",
        "    for i in range(num_windows):\n",
        "        start = i * STRIDE\n",
        "        end = start + WINDOW_SIZE\n",
        "        window = fault_data[start:end]\n",
        "\n",
        "        # Compute statistical features for the current window\n",
        "        mean_val = np.mean(window)\n",
        "        std_val = np.std(window)\n",
        "        rms_val = np.sqrt(np.mean(np.square(window)))\n",
        "        max_val = np.max(window)\n",
        "        min_val = np.min(window)\n",
        "        skewness_val = skew(window)\n",
        "        kurtosis_val = kurtosis(window)\n",
        "\n",
        "        # Additional statistical features\n",
        "        mean_abs_val = np.mean(np.abs(window))\n",
        "        form_factor = rms_val / mean_abs_val if mean_abs_val != 0 else 0\n",
        "        crest_factor = max_val / rms_val if rms_val != 0 else 0\n",
        "\n",
        "        # Store the features in a dictionary\n",
        "        statistical_features.append({\n",
        "            'fault': f,\n",
        "            'window': i + 1,\n",
        "            'mean': mean_val,\n",
        "            'std': std_val,\n",
        "            'rms': rms_val,\n",
        "            'max': max_val,\n",
        "            'min': min_val,\n",
        "            'skewness': skewness_val,\n",
        "            'kurtosis': kurtosis_val,\n",
        "            'form_factor': form_factor,\n",
        "            'crest_factor': crest_factor\n",
        "        })\n",
        "\n",
        "# Convert statistical features into a DataFrame\n",
        "stat_features_df = pd.DataFrame(statistical_features)\n",
        "\n",
        "# Display the extracted features and count of rows\n",
        "print(f\"Extracted {len(stat_features_df)} rows (target: {TARGET_ROWS})\")\n",
        "print(stat_features_df)\n",
        "\n",
        "# Save the extracted features to a CSV file\n",
        "output_file = \"/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_statistical_features_with_form_and_crest_factors.csv\"\n",
        "stat_features_df.to_csv(output_file, index=False)\n",
        "print(f\"Statistical features saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "HRTIq9cYQgjC"
      },
      "id": "HRTIq9cYQgjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
        "df = pd.read_csv('/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_statistical_features_with_form_and_crest_factors.csv')\n",
        "\n",
        "# Display initial class distribution\n",
        "print(\"Initial class distribution:\")\n",
        "print(df['fault'].value_counts())\n",
        "\n",
        "# Target size for undersampling/resampling\n",
        "TARGET_ROWS = 1570\n",
        "\n",
        "# Separate all fault types\n",
        "fault_classes = df['fault'].value_counts()\n",
        "\n",
        "# Determine the number of classes\n",
        "num_classes = len(fault_classes)\n",
        "\n",
        "# Calculate how many samples to take from each class\n",
        "samples_per_class = TARGET_ROWS // num_classes\n",
        "\n",
        "# Initialize a list to store resampled data\n",
        "resampled_data = []\n",
        "\n",
        "# Loop over each class to resample them\n",
        "for fault_class in fault_classes.index:\n",
        "    class_data = df[df['fault'] == fault_class]\n",
        "    if len(class_data) > samples_per_class:\n",
        "        # Undersample\n",
        "        class_data_resampled = resample(class_data, replace=False, n_samples=samples_per_class, random_state=42)\n",
        "    else:\n",
        "        # Upsample\n",
        "        class_data_resampled = resample(class_data, replace=True, n_samples=samples_per_class, random_state=42)\n",
        "\n",
        "    resampled_data.append(class_data_resampled)\n",
        "\n",
        "# Concatenate the resampled data into a single DataFrame\n",
        "balanced_df = pd.concat(resampled_data)\n",
        "\n",
        "# Check if we have the exact number of rows\n",
        "current_rows = len(balanced_df)\n",
        "print(f\"Current rows after equal resampling: {current_rows}\")\n",
        "\n",
        "# If the current rows don't match the target rows, adjust by trimming or adding rows\n",
        "if current_rows > TARGET_ROWS:\n",
        "    # Trim extra rows (if more than target)\n",
        "    balanced_df = balanced_df.head(TARGET_ROWS)\n",
        "elif current_rows < TARGET_ROWS:\n",
        "    # Add rows if fewer than target, by sampling from existing data\n",
        "    remaining_rows = TARGET_ROWS - current_rows\n",
        "    additional_data = balanced_df.sample(n=remaining_rows, replace=True, random_state=42)\n",
        "    balanced_df = pd.concat([balanced_df, additional_data])\n",
        "\n",
        "# Shuffle the final dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display new class distribution\n",
        "print(\"New class distribution:\")\n",
        "print(balanced_df['fault'].value_counts())\n",
        "\n",
        "# Display final row count\n",
        "print(f\"Final row count: {len(balanced_df)}\")\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "balanced_df.to_csv(\"/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_balanced_dataset_equal_resampling_exact_1570.csv\", index=False)\n",
        "print(\"Balanced dataset saved as '/content/drive/MyDrive/MK-DCNN CWRU/NNOISE_0_balanced_dataset_equal_resampling_exact_1570.csv'\")\n"
      ],
      "metadata": {
        "id": "0-DLTbJL0tc6"
      },
      "id": "0-DLTbJL0tc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.shape"
      ],
      "metadata": {
        "id": "NQ3f_WOt7Dld"
      },
      "id": "NQ3f_WOt7Dld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df=balanced_df.sort_values(by='fault')"
      ],
      "metadata": {
        "id": "JP_zDzu6_Zkt"
      },
      "id": "JP_zDzu6_Zkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "id": "eq6MhY1CAvgD"
      },
      "id": "eq6MhY1CAvgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Sort the dataset by the 'Fault' column\n",
        "balanced_df = balanced_df.sort_values(by='fault')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = balanced_df.drop(columns=['fault'])  # Replace 'Fault' with actual target column name\n",
        "y = balanced_df['fault']\n",
        "\n",
        "# Normalize the feature columns using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Concatenate the normalized features with the target variable\n",
        "STAT_Motor_Fault_dataset_normalized = pd.concat([X_normalized, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Sort the normalized dataset by 'Fault' column\n",
        "STAT_Motor_Fault_dataset_normalized = STAT_Motor_Fault_dataset_normalized.sort_values(by='fault')\n",
        "\n",
        "# Apply t-SNE for dimensionality reduction\n",
        "tsne = TSNE(n_components=2, random_state=42)  # 2D t-SNE\n",
        "X_tsne = tsne.fit_transform(X_normalized)\n",
        "\n",
        "# Create a DataFrame for t-SNE results\n",
        "tSNE_representation = pd.DataFrame(X_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
        "tSNE_representation['fault'] = y.reset_index(drop=True)\n",
        "\n",
        "# Display the t-SNE representation\n",
        "print(tSNE_representation.head())\n",
        "\n",
        "# Optional: Visualization of t-SNE representation\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in tSNE_representation['fault'].unique():\n",
        "    subset = tSNE_representation[tSNE_representation['fault'] == label]\n",
        "    plt.scatter(subset['t-SNE 1'], subset['t-SNE 2'], label=f'fault {label}', alpha=0.7)\n",
        "\n",
        "plt.title('t-SNE Representation of Motor Fault Dataset')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tV7aspjE3R4f"
      },
      "id": "tV7aspjE3R4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNE_representation"
      ],
      "metadata": {
        "id": "QMwOLwssDgHU"
      },
      "id": "QMwOLwssDgHU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNE_representation=tSNE_representation.drop(columns=['fault'])\n",
        "tSNE_representation"
      ],
      "metadata": {
        "id": "Pybzwduy8nhv"
      },
      "id": "Pybzwduy8nhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNEdf = tSNEdf.sort_values(by='Fault')"
      ],
      "metadata": {
        "id": "_TpCIloN904b"
      },
      "id": "_TpCIloN904b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNEdf"
      ],
      "metadata": {
        "id": "xq2JlAOp_N5V"
      },
      "id": "xq2JlAOp_N5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Features"
      ],
      "metadata": {
        "id": "73T5xFDWQmML"
      },
      "id": "73T5xFDWQmML"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# Assuming df1 and df2 are your two dataframes\n",
        "# Concatenate the dataframes vertically\n",
        "Motor_Fault_dataset = pd.concat([tSNE_representation, tSNEdf], axis=1)\n",
        "Motor_Fault_dataset=Motor_Fault_dataset.sort_values(by='Fault')\n",
        "Motor_Fault_dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df1 and df2 are your two dataframes\n",
        "# Concatenate the dataframes vertically\n",
        "Motor_Fault_dataset = pd.concat([tSNE_representation, tSNEdf], axis=1)\n",
        "\n",
        "# Sort the dataset by 'Fault' column\n",
        "Motor_Fault_dataset = Motor_Fault_dataset.sort_values(by='Fault')\n",
        "Motor_Fault_dataset\n"
      ],
      "metadata": {
        "id": "sEmeYK5c8anK"
      },
      "id": "sEmeYK5c8anK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report for MKDCNN"
      ],
      "metadata": {
        "id": "zC_6vMBPsOs_"
      },
      "id": "zC_6vMBPsOs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b77LcDP9sPR2"
      },
      "id": "b77LcDP9sPR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HSPSO-CatBoost\n"
      ],
      "metadata": {
        "id": "wZFQ37UCYN1p"
      },
      "id": "wZFQ37UCYN1p"
    },
    {
      "cell_type": "code",
      "source": [
        "X = Motor_Fault_dataset.iloc[:, 0:4].values\n",
        "y = Motor_Fault_dataset.iloc[:, 4].values"
      ],
      "metadata": {
        "id": "f_P8BWzmYTg0"
      },
      "id": "f_P8BWzmYTg0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode categorical labels to numerical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vLMueEZ6YfvA"
      },
      "id": "vLMueEZ6YfvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment:"
      ],
      "metadata": {
        "id": "Ul9AHZU5uIcP"
      },
      "id": "Ul9AHZU5uIcP"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "\n",
        "def _ensure_1d(y):\n",
        "    y = np.array(y)\n",
        "    return y.reshape(-1)\n",
        "\n",
        "\n",
        "X_train_np = np.array(X_train)\n",
        "X_test_np = np.array(X_test)\n",
        "y_train_np = _ensure_1d(y_train)\n",
        "y_test_np = _ensure_1d(y_test)\n",
        "\n",
        "classes = np.unique(y_train_np)\n",
        "LOSS_FUNCTION = \"Logloss\" if len(classes) <= 2 else \"MultiClass\"\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "tr_idx, va_idx = next(sss.split(X_train_np, y_train_np))\n",
        "X_tr, y_tr = X_train_np[tr_idx], y_train_np[tr_idx]\n",
        "X_va, y_va = X_train_np[va_idx], y_train_np[va_idx]\n",
        "\n",
        "\n",
        "search_space = {\n",
        "    \"iterations\": list(range(50, 201, 25)),\n",
        "    \"depth\": list(range(2, 9)),\n",
        "    \"learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
        "    \"l2_leaf_reg\": [1.0, 3.0, 5.0],\n",
        "    \"bagging_temperature\": [0.5, 1.0, 1.5],\n",
        "    \"random_strength\": [0.5, 1.0, 1.5],\n",
        "}\n",
        "keys = list(search_space.keys())\n",
        "\n",
        "\n",
        "def project_to_space(key, value):\n",
        "    choices = np.array(search_space[key], dtype=float)\n",
        "    idx = int(np.argmin(np.abs(choices - float(value))))\n",
        "    v = search_space[key][idx]\n",
        "    if isinstance(search_space[key][0], int):\n",
        "        return int(v)\n",
        "    return float(v)\n",
        "\n",
        "\n",
        "def clip_cast(p):\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        out[k] = project_to_space(k, p[k])\n",
        "    out[\"iterations\"] = int(out[\"iterations\"])\n",
        "    out[\"depth\"] = int(out[\"depth\"])\n",
        "    return out\n",
        "\n",
        "\n",
        "def random_particle(rng):\n",
        "    return {k: rng.choice(search_space[k]) for k in keys}\n",
        "\n",
        "\n",
        "def particle_to_vector(p):\n",
        "    return np.array([float(p[k]) for k in keys], dtype=float)\n",
        "\n",
        "\n",
        "def vector_to_particle(v):\n",
        "    return {k: float(v[i]) for i, k in enumerate(keys)}\n",
        "\n",
        "\n",
        "def objective_fast(params, seed=42):\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=int(params[\"iterations\"]),\n",
        "        depth=int(params[\"depth\"]),\n",
        "        learning_rate=float(params[\"learning_rate\"]),\n",
        "        l2_leaf_reg=float(params[\"l2_leaf_reg\"]),\n",
        "        bagging_temperature=float(params[\"bagging_temperature\"]),\n",
        "        random_strength=float(params[\"random_strength\"]),\n",
        "        loss_function=LOSS_FUNCTION,\n",
        "        eval_metric=\"Accuracy\",\n",
        "        od_type=\"Iter\",\n",
        "        od_wait=20,\n",
        "        verbose=False,\n",
        "        random_seed=seed,\n",
        "        thread_count=-1\n",
        "    )\n",
        "    model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
        "    pred = model.predict(X_va).reshape(-1)\n",
        "    return float(accuracy_score(y_va, pred))\n",
        "\n",
        "\n",
        "def cauchy_mutation(particle, rng, gamma=0.25):\n",
        "    mutated = dict(particle)\n",
        "    for k in keys:\n",
        "        mutated[k] = float(mutated[k]) * (1.0 + gamma * np.tan(np.pi * (rng.random() - 0.5)))\n",
        "        mutated[k] = project_to_space(k, mutated[k])\n",
        "    return clip_cast(mutated)\n",
        "\n",
        "\n",
        "def hs_pso_optimize_fast(\n",
        "    n_part=10,\n",
        "    max_iter=15,\n",
        "    switch_threshold=5,\n",
        "    seed=42,\n",
        "    w_min=0.5, w_max=0.9,\n",
        "    c1=0.8, c2=0.6,\n",
        "    mutation_prob=0.15,\n",
        "    restart_prob=0.08,\n",
        "    target_acc=0.999\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    dim = len(keys)\n",
        "\n",
        "    swarm = [random_particle(rng) for _ in range(n_part)]\n",
        "    vel = [np.zeros(dim, dtype=float) for _ in range(n_part)]\n",
        "\n",
        "    pbest = [dict(s) for s in swarm]\n",
        "    pbest_score = [objective_fast(s, seed=seed) for s in swarm]\n",
        "\n",
        "    gbest_idx = int(np.argmax(pbest_score))\n",
        "    gbest = dict(pbest[gbest_idx])\n",
        "    gbest_score = float(pbest_score[gbest_idx])\n",
        "\n",
        "    topology = \"gbest\"\n",
        "    H = 1.0\n",
        "    b = 0.9\n",
        "\n",
        "    def nonlinear_inertia(it):\n",
        "        numerator = (w_max - w_min) * it\n",
        "        denominator = 1.0 + np.exp(-10.0 * b * ((2.0 * it) / (H * max_iter) - 1.0))\n",
        "        return w_max - numerator / denominator\n",
        "\n",
        "    for it in range(1, max_iter + 1):\n",
        "        if gbest_score >= target_acc:\n",
        "            break\n",
        "\n",
        "        w = nonlinear_inertia(it)\n",
        "\n",
        "        if it % switch_threshold == 0:\n",
        "            topology = \"lbest\" if topology == \"gbest\" else \"gbest\"\n",
        "\n",
        "        scores = []\n",
        "        for i in range(len(swarm)):\n",
        "            s = objective_fast(swarm[i], seed=seed)\n",
        "            scores.append(s)\n",
        "            if s > pbest_score[i]:\n",
        "                pbest[i] = dict(swarm[i])\n",
        "                pbest_score[i] = float(s)\n",
        "\n",
        "        best_i = int(np.argmax(pbest_score))\n",
        "        if pbest_score[best_i] > gbest_score:\n",
        "            gbest = dict(pbest[best_i])\n",
        "            gbest_score = float(pbest_score[best_i])\n",
        "\n",
        "        best_indices = np.argsort(scores)[-max(1, len(swarm)//5):]\n",
        "        for idx in best_indices:\n",
        "            swarm.append(dict(swarm[idx]))\n",
        "            vel.append(vel[idx].copy())\n",
        "            scores.append(scores[idx])\n",
        "            pbest.append(dict(pbest[idx]))\n",
        "            pbest_score.append(float(pbest_score[idx]))\n",
        "\n",
        "        if len(swarm) > 2 * n_part:\n",
        "            worst_indices = np.argsort(scores)[:len(swarm)//4]\n",
        "            for idx in sorted(worst_indices, reverse=True):\n",
        "                del swarm[idx]\n",
        "                del vel[idx]\n",
        "                del scores[idx]\n",
        "                del pbest[idx]\n",
        "                del pbest_score[idx]\n",
        "\n",
        "        for i in range(len(swarm)):\n",
        "            x_vec = particle_to_vector(swarm[i])\n",
        "            p_vec = particle_to_vector(pbest[i])\n",
        "\n",
        "            if topology == \"gbest\":\n",
        "                g_vec = particle_to_vector(gbest)\n",
        "            else:\n",
        "                k = min(5, len(swarm))\n",
        "                neigh = rng.choice(len(swarm), size=k, replace=False)\n",
        "                neigh_best = neigh[np.argmax([pbest_score[j] for j in neigh])]\n",
        "                g_vec = particle_to_vector(pbest[neigh_best])\n",
        "\n",
        "            r1 = rng.random(dim)\n",
        "            r2 = rng.random(dim)\n",
        "\n",
        "            vel[i] = w * vel[i] + c1 * r1 * (p_vec - x_vec) + c2 * r2 * (g_vec - x_vec)\n",
        "            new_vec = x_vec + vel[i]\n",
        "            swarm[i] = clip_cast(vector_to_particle(new_vec))\n",
        "\n",
        "        for i in range(len(swarm)):\n",
        "            if rng.random() < mutation_prob:\n",
        "                swarm[i] = cauchy_mutation(swarm[i], rng, gamma=0.25)\n",
        "\n",
        "        for i in range(len(swarm)):\n",
        "            if rng.random() < restart_prob:\n",
        "                swarm[i] = random_particle(rng)\n",
        "\n",
        "    return gbest, gbest_score\n",
        "\n",
        "\n",
        "start_time = datetime.now()\n",
        "best_params, best_val_acc = hs_pso_optimize_fast()\n",
        "end_time = datetime.now()\n",
        "\n",
        "print(\"Tuning Duration:\", end_time - start_time)\n",
        "print(\"Best Val Accuracy:\", best_val_acc)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "\n",
        "train_pool = Pool(X_train_np, y_train_np)\n",
        "test_pool = Pool(X_test_np, y_test_np)\n",
        "\n",
        "final_model = CatBoostClassifier(\n",
        "    iterations=max(300, int(best_params[\"iterations\"]) * 2),\n",
        "    depth=int(best_params[\"depth\"]),\n",
        "    learning_rate=float(best_params[\"learning_rate\"]),\n",
        "    l2_leaf_reg=float(best_params[\"l2_leaf_reg\"]),\n",
        "    bagging_temperature=float(best_params[\"bagging_temperature\"]),\n",
        "    random_strength=float(best_params[\"random_strength\"]),\n",
        "    loss_function=LOSS_FUNCTION,\n",
        "    eval_metric=\"Accuracy\",\n",
        "    od_type=\"Iter\",\n",
        "    od_wait=40,\n",
        "    verbose=50,\n",
        "    random_seed=42,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "final_model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
        "\n",
        "curve = final_model.get_evals_result()\n",
        "train_acc = curve.get(\"learn\", {}).get(\"Accuracy\", [])\n",
        "val_acc = curve.get(\"validation\", {}).get(\"Accuracy\", [])\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "if train_acc:\n",
        "    plt.plot(train_acc, label=\"Train Accuracy\")\n",
        "if val_acc:\n",
        "    plt.plot(val_acc, label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "y_pred_train = final_model.predict(X_train_np).reshape(-1)\n",
        "y_pred_test = final_model.predict(X_test_np).reshape(-1)\n",
        "\n",
        "print(\"Classification Report - Training Set:\")\n",
        "print(classification_report(y_train_np, y_pred_train))\n",
        "print(\"Classification Report - Test Set:\")\n",
        "print(classification_report(y_test_np, y_pred_test))\n",
        "\n",
        "cm_train = confusion_matrix(y_train_np, y_pred_train)\n",
        "cm_test = confusion_matrix(y_test_np, y_pred_test)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.heatmap(cm_train, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Training Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.heatmap(cm_test, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"C-Matrix.svg\", format=\"svg\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jQr38vF8A9_i"
      },
      "id": "jQr38vF8A9_i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "curve = final_model.get_evals_result()\n",
        "\n",
        "train_acc = curve.get(\"learn\", {}).get(\"Accuracy\", [])\n",
        "val_acc = curve.get(\"validation\", {}).get(\"Accuracy\", [])\n",
        "\n",
        "if not train_acc or not val_acc:\n",
        "    raise ValueError(\"找不到 Accuracy 記錄。請確認 final_model 有設定 eval_metric='Accuracy' 並且 fit 時有給 eval_set。\")\n",
        "\n",
        "x = np.arange(1, len(train_acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.plot(x, train_acc, linestyle=\"--\", marker=\"o\", markersize=3, label=\"accuracy\")\n",
        "plt.plot(x, val_acc, linestyle=\"--\", marker=\"o\", markersize=3, label=\"val_accuracy\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylim(0, 1.01)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "a_9tYmgz45VS"
      },
      "id": "a_9tYmgz45VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 2\n",
        "x = np.arange(1, len(train_acc)+1, step)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(x, np.array(train_acc)[::step],\n",
        "         linestyle=\"--\", marker=\"o\", markersize=4, label=\"accuracy\")\n",
        "plt.plot(x, np.array(val_acc)[::step],\n",
        "         linestyle=\"--\", marker=\"o\", markersize=4, label=\"val_accuracy\")\n",
        "\n",
        "plt.xlabel(\"Iterations(times)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.grid(True)\n",
        "plt.ylim(0.4, 1.05)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5uCypQpV4xZj"
      },
      "id": "5uCypQpV4xZj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve = final_model.get_evals_result()\n",
        "print(learning_curve)"
      ],
      "metadata": {
        "id": "2mRL06xdGZve"
      },
      "id": "2mRL06xdGZve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve evaluation results\n",
        "learning_curve = final_model.get_evals_result()\n",
        "\n",
        "# Extract the metric values\n",
        "# Assuming 'learn' corresponds to the training dataset\n",
        "train_metric_values = learning_curve.get('learn', {}).get('MultiClass', [])\n",
        "\n",
        "# Check the extracted values\n",
        "if train_metric_values:\n",
        "    print(\"Training metric values:\", train_metric_values)\n",
        "else:\n",
        "    print(\"Metric 'MultiClass' not found in evaluation results.\")\n"
      ],
      "metadata": {
        "id": "3baCXa2jGafB"
      },
      "id": "3baCXa2jGafB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the learning curve\n",
        "plt.plot(train_metric_values, label='Training MultiClass Metric')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p0vioY6FGjQb"
      },
      "id": "p0vioY6FGjQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_train = final_model.predict(X_train)\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report - Training Set:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Classification Report - Test Set:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Training Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('C-Matrix.svg', format='svg')\n",
        "plt.show()\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Deployment Duration: {}'.format(end_time - start_time))"
      ],
      "metadata": {
        "id": "EGySyu_OHNqG"
      },
      "id": "EGySyu_OHNqG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}