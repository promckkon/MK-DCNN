{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/promckkon/MK-DCNN/blob/main/MK-DCNN%20with%2020dB%20NOISE%20in%20TRIAX%20Dataset%20250625.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_JTcgOD9s-GR"
      },
      "id": "_JTcgOD9s-GR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1132b641",
      "metadata": {
        "id": "1132b641"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Input"
      ],
      "metadata": {
        "id": "2haV_hKQQOg-"
      },
      "id": "2haV_hKQQOg-"
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(columns=['DE_data','fault']) # upload the dataset on googledrive\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_20db\", topdown=False):\n",
        "    for file_name in files:\n",
        "        path = os.path.join(root, file_name)\n",
        "        print(path)\n",
        "\n",
        "        mat = scipy.io.loadmat(path)\n",
        "\n",
        "        key_name = list(mat.keys())[3]\n",
        "        DE_data = mat.get(key_name)\n",
        "        fault = np.full((len(DE_data), 1), file_name[:-4])\n",
        "\n",
        "        df_temp = pd.DataFrame({'DE_data':np.ravel(DE_data) , 'fault':np.ravel(fault)})\n",
        "\n",
        "        df = pd.concat([df,df_temp],axis=0)\n",
        "        print(df['fault'].unique())\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_faults.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "sXzSVyy_ylsF",
        "outputId": "9c49c4d5-090e-47f2-c421-e7af8171a1e8"
      },
      "id": "sXzSVyy_ylsF",
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F8_Normal_40db.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-3643886688.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df,df_temp],axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F8_Normal_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F7_OR017_40db.mat\n",
            "['F8_Normal_40db' 'F7_OR017_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F1_IR007_40db.mat\n",
            "['F8_Normal_40db' 'F7_OR017_40db' 'F1_IR007_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F2_IR009_40db.mat\n",
            "['F8_Normal_40db' 'F7_OR017_40db' 'F1_IR007_40db' 'F2_IR009_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F3_IR013_40db.mat\n",
            "['F8_Normal_40db' 'F7_OR017_40db' 'F1_IR007_40db' 'F2_IR009_40db'\n",
            " 'F3_IR013_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F4_IR017_40db.mat\n",
            "['F8_Normal_40db' 'F7_OR017_40db' 'F1_IR007_40db' 'F2_IR009_40db'\n",
            " 'F3_IR013_40db' 'F4_IR017_40db']\n",
            "/content/drive/MyDrive/TRIAX_with_NOISE/TRIAX_NOISE_Dataset_40db/F6_OR013_40db.mat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3643886688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mkey_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatFile4Reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_miobase.py\u001b[0m in \u001b[0;36m_get_matfile_version\u001b[0;34m(fileobj)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# Mat4 files have a zero somewhere in first 4 bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mhdr_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_HDR_N_BYTES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr_bytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0m_HDR_N_BYTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMatReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mat file appears to be truncated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Kernel Deep 1D-CNN"
      ],
      "metadata": {
        "id": "2d0arbB2knS8"
      },
      "id": "2d0arbB2knS8"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "-dsJA9B5YVAW"
      },
      "id": "-dsJA9B5YVAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Total target rows\n",
        "TARGET_ROWS = 1800\n",
        "\n",
        "# Estimate the total number of samples across all fault types\n",
        "total_samples = sum(len(df[df['fault'] == f]) for f in df['fault'].unique())\n",
        "\n",
        "# Calculate suitable window size and stride\n",
        "num_faults = 9\n",
        "average_samples_per_fault = total_samples / num_faults\n",
        "approx_windows_per_fault = TARGET_ROWS / num_faults\n",
        "stride_ratio = 0.8  # Initial stride/window ratio\n",
        "\n",
        "win_len = int(average_samples_per_fault / approx_windows_per_fault)\n",
        "stride = int(win_len * stride_ratio)\n",
        "\n",
        "X=[]\n",
        "Y=[]\n",
        "\n",
        "\n",
        "for k in df['fault'].unique():\n",
        "\n",
        "    df_temp_2 = df[df['fault']==k]\n",
        "\n",
        "    for i in np.arange(0,len(df_temp_2)-(win_len),stride):\n",
        "        temp = df_temp_2.iloc[i:i+win_len,:-1].values\n",
        "        temp = temp.reshape((1,-1))\n",
        "        X.append(temp)\n",
        "        Y.append(df_temp_2.iloc[i+win_len,-1])\n",
        "\n",
        "X=np.array(X)\n",
        "X=X.reshape((X.shape[0],-1,1))\n",
        "#X = np.repeat(X, 3, axis=3) # To repeat into 3 chanel format\n",
        "\n",
        "\n",
        "Y=np.array(Y)\n",
        "encoder= LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "OHE_Y = to_categorical(encoded_Y)"
      ],
      "metadata": {
        "id": "5s5vUXrUM2BH"
      },
      "id": "5s5vUXrUM2BH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,OHE_Y,test_size=0.3,shuffle=True)"
      ],
      "metadata": {
        "id": "PreHyG2yMkTA"
      },
      "id": "PreHyG2yMkTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Custom Physics-Informed Loss Function ---\n",
        "def custom_loss(y_true, y_pred):\n",
        "    # Standard categorical crossentropy\n",
        "    loss = K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Physics-Informed Term: penalize rapid class probability changes\n",
        "    # Ensure y_pred is at least 2D for slicing\n",
        "    if K.ndim(y_pred) < 2:\n",
        "        y_pred = K.expand_dims(y_pred, axis=-1)\n",
        "\n",
        "    # Ensure y_pred has more than one class dimension to compute diff\n",
        "    if K.int_shape(y_pred)[-1] > 1:\n",
        "        diff = y_pred[:, 1:] - y_pred[:, :-1]\n",
        "        squared_diff = tf.square(diff)\n",
        "        physics_term = tf.reduce_mean(squared_diff)\n",
        "    else:\n",
        "        physics_term = 0.0 # No physics term if only one class\n",
        "\n",
        "\n",
        "    # Total loss = classification loss + regularization term\n",
        "    total_loss = loss + 0.01 * physics_term  # 0.01 is tunable\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# --- Model Definition ---\n",
        "# no_classes = len(df['fault'].unique()) # Original line that caused 10 classes\n",
        "no_classes = len(encoder.classes_) # Use the encoder to get the correct number of classes\n",
        "print(f\"Number of output classes (no_classes): {no_classes}\") # Add print statement to verify\n",
        "\n",
        "input_shape = (X.shape[1], X.shape[2])  # Example: (784, 1)\n",
        "\n",
        "# Head 1\n",
        "inputs1 = Input(shape=input_shape)\n",
        "conv1 = Conv1D(filters=64, kernel_size=200, activation='relu')(inputs1)\n",
        "drop1 = Dropout(0.5)(conv1)\n",
        "pool1 = MaxPooling1D(pool_size=20)(drop1)\n",
        "flat1 = Flatten()(pool1)\n",
        "\n",
        "# Head 2\n",
        "inputs2 = Input(shape=input_shape)\n",
        "conv2 = Conv1D(filters=64, kernel_size=100, activation='relu')(inputs2)\n",
        "drop2 = Dropout(0.5)(conv2)\n",
        "pool2 = MaxPooling1D(pool_size=10)(drop2)\n",
        "flat2 = Flatten()(pool2)\n",
        "\n",
        "# Head 3\n",
        "inputs3 = Input(shape=input_shape)\n",
        "conv3 = Conv1D(filters=64, kernel_size=50, activation='relu')(inputs3)\n",
        "drop3 = Dropout(0.5)(conv3)\n",
        "pool3 = MaxPooling1D(pool_size=5)(drop3)\n",
        "flat3 = Flatten()(pool3)\n",
        "\n",
        "# Merge all feature paths\n",
        "merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "# Fully connected interpretation\n",
        "dense1 = Dense(100, activation='relu')(merged)\n",
        "outputs = Dense(no_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Build and compile model with custom loss\n",
        "cnn_model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "cnn_model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "kB3uGh-8J7no"
      },
      "id": "kB3uGh-8J7no",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =100\n",
        "epochs = 20\n",
        "history = cnn_model.fit([X_train,X_train,X_train], y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=([X_test,X_test,X_test],y_test),shuffle=True)"
      ],
      "metadata": {
        "id": "2Lxm-qvKM3oN"
      },
      "id": "2Lxm-qvKM3oN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inv_Transform_result(y_pred):\n",
        "    y_pred = y_pred.argmax(axis=1)\n",
        "    y_pred = encoder.inverse_transform(y_pred)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "\n",
        "y_pred=cnn_model.predict([X_test,X_test,X_test])\n",
        "\n",
        "\n",
        "Y_pred=inv_Transform_result(y_pred)\n",
        "Y_test = inv_Transform_result(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "cm = confusion_matrix(Y_test, Y_pred,normalize='true')\n",
        "f = sns.heatmap(cm, annot=True,xticklabels=encoder.classes_,yticklabels=encoder.classes_)"
      ],
      "metadata": {
        "id": "pbBmkBjlM5ks"
      },
      "id": "pbBmkBjlM5ks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cnn = Model(inputs=cnn_model.input,outputs=cnn_model.layers[16].output)\n",
        "y_viz = dummy_cnn.predict([X_train,X_train,X_train])"
      ],
      "metadata": {
        "id": "3tgRx2TxM-GS"
      },
      "id": "3tgRx2TxM-GS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Apply t-SNE transformation\n",
        "X_t_sne = TSNE(\n",
        "    n_components=2,\n",
        "    learning_rate='auto',\n",
        "    verbose=2,  # Increase verbosity for more detailed feedback\n",
        "    perplexity=40,\n",
        "    n_iter=500  # Slightly increased number of iterations for convergence\n",
        ").fit_transform(y_viz)\n",
        "\n",
        "# Create DataFrame for t-SNE components\n",
        "tSNEdf = pd.DataFrame(data=X_t_sne, columns=['T-SNE component 1', 'T-SNE component 2'])\n",
        "\n",
        "# Assuming `inv_Transform_result` transforms `y_train` to get the Fault labels\n",
        "tSNEdf['Fault'] = inv_Transform_result(y_train)\n",
        "\n",
        "# Save the t-SNE DataFrame as a CSV file\n",
        "tSNEdf.to_csv('/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_tSNE_results.csv', index=True)\n",
        "print(\"t-SNE results saved as '/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_tSNE_results.csv'.\")\n",
        "\n",
        "# # Plot the t-SNE results with Fault as hue\n",
        "# plt.figure(figsize=(12, 12))  # Larger figure for clarity\n",
        "# sns.scatterplot(\n",
        "#     x='T-SNE component 1',\n",
        "#     y='T-SNE component 2',\n",
        "#     hue='Fault',\n",
        "#     palette=sns.color_palette(\"husl\", as_cmap=False),  # Vivid color palette\n",
        "#     data=tSNEdf,\n",
        "#     legend=\"full\",\n",
        "#     alpha=0.7,  # Increased transparency for better overlap clarity\n",
        "#     s=100  # Larger marker size\n",
        "# )\n",
        "\n",
        "# # Add gridlines for better visualization\n",
        "# plt.grid\n",
        "\n",
        "\n",
        "# Optional: Visualization of t-SNE representation\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in tSNEdf['Fault'].unique():\n",
        "    subset = tSNEdf[tSNEdf['Fault'] == label]\n",
        "    plt.scatter(subset['T-SNE component 1'], subset['T-SNE component 2'], label=f'Fault {label}', alpha=0.7)\n",
        "\n",
        "plt.title('t-SNE Representation of Motor Fault Dataset')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uic9AVV2NBah"
      },
      "id": "uic9AVV2NBah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Features"
      ],
      "metadata": {
        "id": "PRuXPOIH6cTH"
      },
      "id": "PRuXPOIH6cTH"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Total target rows\n",
        "TARGET_ROWS = 1800\n",
        "\n",
        "# Estimate the total number of samples across all fault types\n",
        "total_samples = sum(len(df[df['fault'] == f]) for f in df['fault'].unique())\n",
        "\n",
        "# Calculate suitable window size and stride\n",
        "num_faults = 9\n",
        "average_samples_per_fault = total_samples / num_faults\n",
        "approx_windows_per_fault = TARGET_ROWS / num_faults\n",
        "stride_ratio = 0.8  # Initial stride/window ratio\n",
        "\n",
        "WINDOW_SIZE = int(average_samples_per_fault / approx_windows_per_fault)\n",
        "STRIDE = int(WINDOW_SIZE * stride_ratio)\n",
        "\n",
        "print(f\"Calculated WINDOW_SIZE: {WINDOW_SIZE}, STRIDE: {STRIDE}\")\n",
        "\n",
        "# Initialize a list to store statistical features\n",
        "statistical_features = []\n",
        "\n",
        "# Loop through each fault type\n",
        "for f in df['fault'].unique():\n",
        "    # Extract data for the current fault type\n",
        "    fault_data = df[df['fault'] == f].iloc[:, 0].values  # Convert to NumPy array for efficiency\n",
        "\n",
        "    # Compute windows using sliding window approach\n",
        "    num_windows = (len(fault_data) - WINDOW_SIZE) // STRIDE + 1\n",
        "    for i in range(num_windows):\n",
        "        start = i * STRIDE\n",
        "        end = start + WINDOW_SIZE\n",
        "        window = fault_data[start:end]\n",
        "\n",
        "        # Convert window to a numeric type\n",
        "        window = window.astype(float)\n",
        "\n",
        "        # Compute statistical features for the current window\n",
        "        mean_val = np.mean(window)\n",
        "        std_val = np.std(window)\n",
        "        rms_val = np.sqrt(np.mean(np.square(window)))\n",
        "        max_val = np.max(window)\n",
        "        min_val = np.min(window)\n",
        "        skewness_val = skew(window)\n",
        "        kurtosis_val = kurtosis(window)\n",
        "\n",
        "        # Additional statistical features\n",
        "        mean_abs_val = np.mean(np.abs(window))\n",
        "        form_factor = rms_val / mean_abs_val if mean_abs_val != 0 else 0\n",
        "        crest_factor = max_val / rms_val if rms_val != 0 else 0\n",
        "\n",
        "        # Store the features in a dictionary\n",
        "        statistical_features.append({\n",
        "            'fault': f,\n",
        "            'window': i + 1,\n",
        "            'mean': mean_val,\n",
        "            'std': std_val,\n",
        "            'rms': rms_val,\n",
        "            'max': max_val,\n",
        "            'min': min_val,\n",
        "            'skewness': skewness_val,\n",
        "            'kurtosis': kurtosis_val,\n",
        "            'form_factor': form_factor,\n",
        "            'crest_factor': crest_factor\n",
        "        })\n",
        "\n",
        "# Convert statistical features into a DataFrame\n",
        "stat_features_df = pd.DataFrame(statistical_features)\n",
        "\n",
        "# Display the extracted features and count of rows\n",
        "print(f\"Extracted {len(stat_features_df)} rows (target: {TARGET_ROWS})\")\n",
        "print(stat_features_df)\n",
        "\n",
        "# Save the extracted features to a CSV file\n",
        "output_file = \"/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_statistical_features_with_form_and_crest_factors.csv\"\n",
        "stat_features_df.to_csv(output_file, index=False)\n",
        "print(f\"Statistical features saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "HRTIq9cYQgjC"
      },
      "id": "HRTIq9cYQgjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
        "df = pd.read_csv('/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_statistical_features_with_form_and_crest_factors.csv')\n",
        "\n",
        "# Display initial class distribution\n",
        "print(\"Initial class distribution:\")\n",
        "print(df['fault'].value_counts())\n",
        "\n",
        "# Target size for undersampling/resampling\n",
        "TARGET_ROWS = 1570\n",
        "\n",
        "# Separate all fault types\n",
        "fault_classes = df['fault'].value_counts()\n",
        "\n",
        "# Determine the number of classes\n",
        "num_classes = len(fault_classes)\n",
        "\n",
        "# Calculate how many samples to take from each class\n",
        "samples_per_class = TARGET_ROWS // num_classes\n",
        "\n",
        "# Initialize a list to store resampled data\n",
        "resampled_data = []\n",
        "\n",
        "# Loop over each class to resample them\n",
        "for fault_class in fault_classes.index:\n",
        "    class_data = df[df['fault'] == fault_class]\n",
        "    if len(class_data) > samples_per_class:\n",
        "        # Undersample\n",
        "        class_data_resampled = resample(class_data, replace=False, n_samples=samples_per_class, random_state=42)\n",
        "    else:\n",
        "        # Upsample\n",
        "        class_data_resampled = resample(class_data, replace=True, n_samples=samples_per_class, random_state=42)\n",
        "\n",
        "    resampled_data.append(class_data_resampled)\n",
        "\n",
        "# Concatenate the resampled data into a single DataFrame\n",
        "balanced_df = pd.concat(resampled_data)\n",
        "\n",
        "# Check if we have the exact number of rows\n",
        "current_rows = len(balanced_df)\n",
        "print(f\"Current rows after equal resampling: {current_rows}\")\n",
        "\n",
        "# If the current rows don't match the target rows, adjust by trimming or adding rows\n",
        "if current_rows > TARGET_ROWS:\n",
        "    # Trim extra rows (if more than target)\n",
        "    balanced_df = balanced_df.head(TARGET_ROWS)\n",
        "elif current_rows < TARGET_ROWS:\n",
        "    # Add rows if fewer than target, by sampling from existing data\n",
        "    remaining_rows = TARGET_ROWS - current_rows\n",
        "    additional_data = balanced_df.sample(n=remaining_rows, replace=True, random_state=42)\n",
        "    balanced_df = pd.concat([balanced_df, additional_data])\n",
        "\n",
        "# Shuffle the final dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display new class distribution\n",
        "print(\"New class distribution:\")\n",
        "print(balanced_df['fault'].value_counts())\n",
        "\n",
        "# Display final row count\n",
        "print(f\"Final row count: {len(balanced_df)}\")\n",
        "\n",
        "# Save the balanced dataset to a CSV file\n",
        "balanced_df.to_csv(\"/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_balanced_dataset_equal_resampling_exact_1570.csv\", index=False)\n",
        "print(\"Balanced dataset saved as '/content/drive/MyDrive/MK DCNN TRIAX/NOISE_20_balanced_dataset_equal_resampling_exact_1570.csv'\")\n"
      ],
      "metadata": {
        "id": "0-DLTbJL0tc6"
      },
      "id": "0-DLTbJL0tc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.shape"
      ],
      "metadata": {
        "id": "NQ3f_WOt7Dld"
      },
      "id": "NQ3f_WOt7Dld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df=balanced_df.sort_values(by='fault')"
      ],
      "metadata": {
        "id": "JP_zDzu6_Zkt"
      },
      "id": "JP_zDzu6_Zkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "id": "eq6MhY1CAvgD"
      },
      "id": "eq6MhY1CAvgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Sort the dataset by the 'Fault' column\n",
        "balanced_df = balanced_df.sort_values(by='fault')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = balanced_df.drop(columns=['fault'])  # Replace 'Fault' with actual target column name\n",
        "y = balanced_df['fault']\n",
        "\n",
        "# Normalize the feature columns using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Concatenate the normalized features with the target variable\n",
        "STAT_Motor_Fault_dataset_normalized = pd.concat([X_normalized, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Sort the normalized dataset by 'Fault' column\n",
        "STAT_Motor_Fault_dataset_normalized = STAT_Motor_Fault_dataset_normalized.sort_values(by='fault')\n",
        "\n",
        "# Apply t-SNE for dimensionality reduction\n",
        "tsne = TSNE(n_components=2, random_state=42)  # 2D t-SNE\n",
        "X_tsne = tsne.fit_transform(X_normalized)\n",
        "\n",
        "# Create a DataFrame for t-SNE results\n",
        "tSNE_representation = pd.DataFrame(X_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
        "tSNE_representation['fault'] = y.reset_index(drop=True)\n",
        "\n",
        "# Display the t-SNE representation\n",
        "print(tSNE_representation.head())\n",
        "\n",
        "# Optional: Visualization of t-SNE representation\n",
        "plt.figure(figsize=(8, 6))\n",
        "for label in tSNE_representation['fault'].unique():\n",
        "    subset = tSNE_representation[tSNE_representation['fault'] == label]\n",
        "    plt.scatter(subset['t-SNE 1'], subset['t-SNE 2'], label=f'fault {label}', alpha=0.7)\n",
        "\n",
        "plt.title('t-SNE Representation of Motor Fault Dataset')\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tV7aspjE3R4f"
      },
      "id": "tV7aspjE3R4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNE_representation"
      ],
      "metadata": {
        "id": "QMwOLwssDgHU"
      },
      "id": "QMwOLwssDgHU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNE_representation=tSNE_representation.drop(columns=['fault'])\n",
        "tSNE_representation"
      ],
      "metadata": {
        "id": "Pybzwduy8nhv"
      },
      "id": "Pybzwduy8nhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNEdf = tSNEdf.sort_values(by='Fault')"
      ],
      "metadata": {
        "id": "_TpCIloN904b"
      },
      "id": "_TpCIloN904b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tSNEdf"
      ],
      "metadata": {
        "id": "xq2JlAOp_N5V"
      },
      "id": "xq2JlAOp_N5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Features"
      ],
      "metadata": {
        "id": "73T5xFDWQmML"
      },
      "id": "73T5xFDWQmML"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# Assuming df1 and df2 are your two dataframes\n",
        "# Concatenate the dataframes vertically\n",
        "Motor_Fault_dataset = pd.concat([tSNE_representation, tSNEdf], axis=1)\n",
        "Motor_Fault_dataset=Motor_Fault_dataset.sort_values(by='Fault')\n",
        "Motor_Fault_dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df1 and df2 are your two dataframes\n",
        "# Concatenate the dataframes vertically\n",
        "Motor_Fault_dataset = pd.concat([tSNE_representation, tSNEdf], axis=1)\n",
        "\n",
        "# Sort the dataset by 'Fault' column\n",
        "Motor_Fault_dataset = Motor_Fault_dataset.sort_values(by='Fault')\n",
        "Motor_Fault_dataset\n"
      ],
      "metadata": {
        "id": "sEmeYK5c8anK"
      },
      "id": "sEmeYK5c8anK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report for MKDCNN"
      ],
      "metadata": {
        "id": "zC_6vMBPsOs_"
      },
      "id": "zC_6vMBPsOs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b77LcDP9sPR2"
      },
      "id": "b77LcDP9sPR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HSPSO-CatBoost\n"
      ],
      "metadata": {
        "id": "wZFQ37UCYN1p"
      },
      "id": "wZFQ37UCYN1p"
    },
    {
      "cell_type": "code",
      "source": [
        "X = Motor_Fault_dataset.iloc[:, 0:4].values\n",
        "y = Motor_Fault_dataset.iloc[:, 4].values"
      ],
      "metadata": {
        "id": "f_P8BWzmYTg0"
      },
      "id": "f_P8BWzmYTg0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "vLMueEZ6YfvA"
      },
      "id": "vLMueEZ6YfvA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hyperactive"
      ],
      "metadata": {
        "id": "nsTDebhD9iQw"
      },
      "id": "nsTDebhD9iQw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "UTH39uc29p_Q"
      },
      "id": "UTH39uc29p_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployment:"
      ],
      "metadata": {
        "id": "Ul9AHZU5uIcP"
      },
      "id": "Ul9AHZU5uIcP"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from hyperactive import Hyperactive\n",
        "from hyperactive.optimizers import ParticleSwarmOptimizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === DATA: Replace X_train, X_test, y_train, y_test with your actual data ===\n",
        "# Example:\n",
        "# X_train, X_test, y_train, y_test = train_test_split(...)\n",
        "\n",
        "X, y = X_train, y_train\n",
        "\n",
        "# === Objective Function ===\n",
        "def model(opt):\n",
        "    CatBoost = CatBoostClassifier(\n",
        "        iterations=opt[\"iterations\"],\n",
        "        depth=opt[\"depth\"],\n",
        "        learning_rate=opt[\"learning_rate\"],\n",
        "        l2_leaf_reg=opt[\"l2_leaf_reg\"],\n",
        "        bagging_temperature=opt[\"bagging_temperature\"],\n",
        "        random_strength=opt[\"random_strength\"],\n",
        "        verbose=False\n",
        "    )\n",
        "    scores = cross_val_score(CatBoost, X, y, cv=4)\n",
        "    return scores.mean()\n",
        "\n",
        "# === Search Space ===\n",
        "search_space = {\n",
        "    \"iterations\": list(range(50, 501, 50)),\n",
        "    \"depth\": list(range(1, 11)),\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "    \"l2_leaf_reg\": [1, 3, 5],\n",
        "    \"bagging_temperature\": [0.5, 1, 1.5],\n",
        "    \"random_strength\": [0.5, 1, 1.5],\n",
        "}\n",
        "\n",
        "# === Cauchy Mutation ===\n",
        "def cauchy_mutation(particle, gamma=0.3):\n",
        "    for key in particle:\n",
        "        if isinstance(particle[key], (int, float)):\n",
        "            particle[key] = particle[key] * (1 + gamma * np.tan(np.pi * (np.random.rand() - 0.5)))\n",
        "            particle[key] = np.clip(particle[key], min(search_space[key]), max(search_space[key]))\n",
        "            if isinstance(search_space[key][0], int):\n",
        "                particle[key] = int(round(particle[key]))\n",
        "    return particle\n",
        "\n",
        "# === HSPSO Optimizer Class ===\n",
        "class HybridStrategyPSO(ParticleSwarmOptimizer):\n",
        "    def __init__(self, n_part=20, max_iter=40, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.switch_threshold = 10\n",
        "        self.iteration = 0\n",
        "        self.topology = \"gbest\"\n",
        "        self.n_part = n_part\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        # Adaptive Inertia Parameters\n",
        "        self.H = 1\n",
        "        self.w_min = 0.4\n",
        "        self.w_max = 0.9\n",
        "        self.b = 0.9\n",
        "        self.scaling_factor = 0.5\n",
        "\n",
        "    def on_iteration(self, swarm, scores):\n",
        "        self.iteration += 1\n",
        "\n",
        "        # === Topology Switching ===\n",
        "        if self.iteration % self.switch_threshold == 0:\n",
        "            self.topology = \"lbest\" if self.topology == \"gbest\" else \"gbest\"\n",
        "            self.set_topology(self.topology)\n",
        "\n",
        "        # === Clone Best Particles ===\n",
        "        best_indices = np.argsort(scores)[:max(1, len(swarm) // 5)]\n",
        "        for idx in best_indices:\n",
        "            particle = swarm[idx].copy()\n",
        "            swarm.append(particle)\n",
        "            scores.append(scores[idx])\n",
        "\n",
        "        # === Prune Worst Particles ===\n",
        "        if len(swarm) > 2 * self.n_part:\n",
        "            worst_indices = np.argsort(scores)[-len(swarm)//4:]\n",
        "            for i in reversed(worst_indices):\n",
        "                del swarm[i]\n",
        "                del scores[i]\n",
        "\n",
        "        # === Nonlinear Adaptive Inertia ===\n",
        "        numerator = (self.w_max - self.w_min) * self.iteration\n",
        "        denominator = 1 + np.exp(-10 * self.b * ((2 * self.iteration) / (self.H * self.max_iter) - 1))\n",
        "        self.inertia = self.w_max - numerator / denominator\n",
        "\n",
        "        # === Cauchy Mutation ===\n",
        "        for i in range(len(swarm)):\n",
        "            if np.random.rand() < 0.2:\n",
        "                swarm[i] = cauchy_mutation(swarm[i])\n",
        "\n",
        "        # === Hook-Jeeves Local Search (simplified) ===\n",
        "        top_ids = np.argsort(scores)[:2]\n",
        "        for i in top_ids:\n",
        "            particle = swarm[i]\n",
        "            for key in particle:\n",
        "                if isinstance(particle[key], (int, float)):\n",
        "                    for delta in [-0.01, 0.01] if isinstance(particle[key], float) else [-1, 1]:\n",
        "                        trial = particle.copy()\n",
        "                        trial[key] = trial[key] + delta\n",
        "                        trial[key] = np.clip(trial[key], min(search_space[key]), max(search_space[key]))\n",
        "                        if isinstance(search_space[key][0], int):\n",
        "                            trial[key] = int(round(trial[key]))\n",
        "                        trial_score = model(trial)\n",
        "                        if trial_score > scores[i]:\n",
        "                            swarm[i] = trial\n",
        "                            scores[i] = trial_score\n",
        "\n",
        "        # === Random Restart ===\n",
        "        if np.random.rand() < 0.1:\n",
        "            random_idx = np.random.choice(len(swarm))\n",
        "            for key in swarm[random_idx]:\n",
        "                if isinstance(swarm[random_idx][key], (int, float)):\n",
        "                    swarm[random_idx][key] = np.random.choice(search_space[key])\n",
        "\n",
        "# === Run HSPSO Hyperparameter Tuning ===\n",
        "start_time = datetime.now()\n",
        "\n",
        "hyper = Hyperactive()\n",
        "\n",
        "optimizer = HybridStrategyPSO(\n",
        "    n_part=20,\n",
        "    max_iter=40,\n",
        "    inertia=0.5,\n",
        "    cognitive_weight=0.9,\n",
        "    social_weight=0.5,\n",
        "    temp_weight=0.4,\n",
        "    rand_rest_p=0.05,\n",
        ")\n",
        "\n",
        "hyper.add_search(model, search_space, optimizer=optimizer, n_iter=40)\n",
        "hyper.run()\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Tuning Duration:', end_time - start_time)\n",
        "\n",
        "# === Best Parameters ===\n",
        "best_params = hyper.best_para(model)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# === Train Final Model with Best Parameters ===\n",
        "train_pool = Pool(data=X_train, label=y_train)\n",
        "test_pool = Pool(data=X_test, label=y_test)\n",
        "\n",
        "final_model = CatBoostClassifier(\n",
        "    depth=best_params[\"depth\"],\n",
        "    iterations=best_params[\"iterations\"],\n",
        "    learning_rate=best_params[\"learning_rate\"],\n",
        "    l2_leaf_reg=best_params[\"l2_leaf_reg\"],\n",
        "    bagging_temperature=best_params[\"bagging_temperature\"],\n",
        "    random_strength=best_params[\"random_strength\"],\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    train_pool,\n",
        "    eval_set=test_pool,\n",
        "    verbose=best_params[\"iterations\"] // 10,\n",
        "    plot=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "jQr38vF8A9_i"
      },
      "id": "jQr38vF8A9_i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyper.add_search(model, search_space, optimizer=optimizer, n_iter=40, initialize={\"random\": 20})\n"
      ],
      "metadata": {
        "id": "a_9tYmgz45VS"
      },
      "id": "a_9tYmgz45VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve = final_model.get_evals_result()\n",
        "print(learning_curve)"
      ],
      "metadata": {
        "id": "2mRL06xdGZve"
      },
      "id": "2mRL06xdGZve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve evaluation results\n",
        "learning_curve = final_model.get_evals_result()\n",
        "\n",
        "# Extract the metric values\n",
        "# Assuming 'learn' corresponds to the training dataset\n",
        "train_metric_values = learning_curve.get('learn', {}).get('MultiClass', [])\n",
        "\n",
        "# Check the extracted values\n",
        "if train_metric_values:\n",
        "    print(\"Training metric values:\", train_metric_values)\n",
        "else:\n",
        "    print(\"Metric 'MultiClass' not found in evaluation results.\")\n"
      ],
      "metadata": {
        "id": "3baCXa2jGafB"
      },
      "id": "3baCXa2jGafB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the learning curve\n",
        "plt.plot(train_metric_values, label='Training MultiClass Metric')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p0vioY6FGjQb"
      },
      "id": "p0vioY6FGjQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_train = final_model.predict(X_train)\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report - Training Set:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Classification Report - Test Set:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Training Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('C-Matrix.svg', format='svg')\n",
        "plt.show()\n",
        "\n",
        "end_time = datetime.now()\n",
        "print('Deployment Duration: {}'.format(end_time - start_time))"
      ],
      "metadata": {
        "id": "EGySyu_OHNqG"
      },
      "id": "EGySyu_OHNqG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
        "\n",
        "# Evaluate train accuracy\n",
        "result_train = final_model.score(X_train, y_train)\n",
        "print(\"Train Accuracy : {}\".format(result_train))\n",
        "\n",
        "# Evaluate test accuracy\n",
        "result_test = final_model.score(X_test, y_test)\n",
        "print(\"Test Accuracy : {}\".format(result_test))\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = final_model.predict(X_train)\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report - Training Set:\")\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "print(\"Classification Report - Test Set:\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "tEOwmxFROH5f"
      },
      "id": "tEOwmxFROH5f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}